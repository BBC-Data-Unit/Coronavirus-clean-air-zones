---
title: "Air pollution and coronavirus"
output: html_notebook
---

# Analysing the impact of coronavirus on air pollution 

This section of code below is written by Anna Khoo. First we import the data, which is in two parts and needs combining:

```{r import data}
#Load the packages we need
pacman::p_load(tidyverse, janitor, ggplot2, sf,rgdal, rgeos)
#import, collate and tidy:

#read in both parts of monitoring data (DEFRA portal maxes out on number of columns allowed)
#NB that R can struggle to read in data 'as is' in downloaded format
#remove first two header rows in Excel before import to help it read csv properly:

daily_data_part_1 <- read_csv("daily_mean_mar_19_to_24_03.csv")
daily_data_part_2 <- read_csv("daily_mean_mar_19_to_24_03_part_2.csv")

daily_data <- cbind(daily_data_part_1, daily_data_part_2)
#remove the parts no longer needed
rm(daily_data_part_1, daily_data_part_2)

```

The resulting file has a lot of fields, and place data is stored in them.

```{r show colnames}
#Show the first few column names
head(colnames(daily_data))
```

We also have headings in the second row:

```{r show first rows}
#Show the first few rows of data
head(daily_data)
```

## Clean the data 

First we need to remove a bunch of columns that repeat the same information about the unit of measurement.

```{r filter out cols}
#generate a T/F vector that indicates whether, for each column, row 1 has either of the values indicated.
daily_data_subset <- daily_data[1,] %in% c("Date", "Nitrogen dioxide")
#Use that T/F vector to filter the data so it only has those columns
daily_data <- daily_data[daily_data_subset]
#Then remove that first row
daily_data <- daily_data[-1,]
#Remove the vector now it's been used
rm(daily_data_subset)
```

We also do some more cleaning, removing empty rows, a duplicate column, and rename the first column.

```{r remove empty rows}
#remove empty rows
daily_data <- daily_data %>% 
  remove_empty("rows")
#remove a column called X1.1 which duplicates the date column X1
daily_data <- daily_data %>% 
  select(-`X1.1`)
#And rename column 1
daily_data <- daily_data %>% 
  rename("Date"=X1)
#Now show the headings
names(daily_data)
```

## Reshape the data so column names become data

Our biggest problem is that the column names should be used as data. We use `pivot_longer` to turn this wide shape into a long shape where the column names from column 2 to 167 (the last column) are used to fill a 'Station' field, and the values in those columns to a field called 'Reading'.

```{r reshape wide to long}
#reshape:
daily_data_long <- daily_data %>% 
  pivot_longer(cols=2:166,
               names_to="Station", 
               values_to="Reading")
```

## Format data as numeric

Our data is all characters...

```{r summarise}
summary(daily_data_long)
```

So we format as numeric:

```{r convert to numeric}
#Reading column is still char, convert
daily_data_long$Reading <- as.numeric(daily_data_long$Reading)
#Check
summary(daily_data_long)
```

## Remove empty data

Some rows have a date as 'end' and no readings. We remove those.

```{r remove empty data}
#filter to leave entries where the Date column is not 'End'
daily_data_long <- daily_data_long %>% 
  filter(!Date=="End")
```

We also remove those with NA readings - and rename that column to *mean* reading.

```{r remove NA}
#filter to leave entries where the Date column is not 'End'
daily_data_long <- daily_data_long %>% 
  filter(!is.na(Reading))
colnames(daily_data_long)[3] <- "mean_reading"
#And export
write.csv(daily_data_long, "daily_data_long.csv")
```

## Filter to 3 cities

We want to limit to Birmingham, Oxford and Bristol so we need to find out which stations include those.

```{r find stations in cities}
#Create a vector of stations
stationlist <- unique(daily_data_long$Station)
#identify the positions where those appear
grep("Birmingham|Oxford|Bristol", stationlist)
#Use that to extract matching stations
cities <- stationlist[grep("Birmingham|Oxford|Bristol|Leeds|Sheffield|Reading|Newcastle", stationlist)]
#Show the results
cities
```

Oldbury is in Sandwell, not Birmingham. But we can use the rest to create a vector and match against that:

```{r limit to cities}
#Remove Oldbury at position 9
cities <- cities[-9]
#Show the results
cities
#filter to leave entries where the Date column is not 'End'
daily_data_long.cities <- daily_data_long %>% 
  filter(Station %in% cities)
```

Let's also categorise each city's stations so we can aggregate:

```{r extract city}
#We use some regex from https://stackoverflow.com/questions/31925811/extract-first-word-from-a-column-and-insert-into-new-column/31925893
daily_data_long.cities$city  <- sub("([A-Za-z]+).*", "\\1",daily_data_long.cities$Station," ")
```



## March only 

We are interested in comparing March this year and last, so let's create a filtered dataset for that:

```{r extract month and filter}
#Add columns for day, month and year
daily_data_long.cities$day <- substr(daily_data_long.cities$Date,1,2)
daily_data_long.cities$month <- substr(daily_data_long.cities$Date,4,5)
daily_data_long.cities$year <- substr(daily_data_long.cities$Date,7,10)
#Filter where that's not March
daily_data_long.march <- daily_data_long.cities %>% 
  filter(month=="03")
#And filter out days after the 24th, as we don't have those for 2020
#First, convert to numeric
daily_data_long.march$day <- as.numeric(daily_data_long.march$day)
#Filter to only include days before the 27th
daily_data_long.march <- daily_data_long.march %>% 
  filter(day<27)
#Export
write_csv(daily_data_long.march,"daily_data_long.march.csv")
```


## Pivot data 

This is remaining code from Anna (some of it superseded by that above)

```{r}

```


```{r}

day_pivot <- daily_data_long.cities %>% 
  group_by(Station) %>% 
  summarise(day_avg=mean(mean_reading, na.rm=T)) %>% 
  arrange(year)
```




```{r}

ggplot(day_pivot) +
  aes(x = Date, weight = day_avg) +
  geom_bar(fill = "#0c4c8a") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

#compare this march with last march
march_compare <- daily_data %>% 
  filter(str_detect(Date, "/03/"))

#create function to mimic RIGHT() function in Excel to extract year value from date
#(lubridate requires yyyy/mm/dd format or similar to auto-parse, this is quicker here)
substrRight <- function(x, n){ substr(x, nchar(x)-n+1, nchar(x)) } 

march_compare <- march_compare %>% 
  mutate(year=substrRight(Date,4))

march_compare_agg <- march_compare %>% 
  group_by(Date, year) %>% 
  summarise(day_avg=sum(Reading, na.rm=T)) %>% 
  arrange(Date)

marches <- ggplot(march_compare_agg, aes(x=Date, y=day_avg)) +
  facet_wrap(~year)+
  geom_bar(stat="identity",
         position="identity",
         fill="#1380A1")+
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
marches

marches <- ggplot(march_compare, aes(x=Date, y=Reading)) +
  facet_wrap(~year)+
  geom_boxplot(fill="#1380A1")+
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
marches

#argh facets aren't clean for some reason. Filter and plot separately:

march_19 <- march_compare %>% 
  filter(year==2019)
march_20 <- march_compare %>% 
  filter(year==2020)

plot_march_19 <- ggplot(march_19, aes(x=Date, y=Reading))+
  geom_boxplot(fill="#1380A1")+
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
plot_march_19

plot_march_20 <- ggplot(march_20, aes(x=Date, y=Reading))+
  geom_boxplot(fill="#1380A1")+
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
plot_march_20

#add in lookups for region/la/environment type:

info_about_monitoring_stations <- read_csv("Defra info about monitoring stations.xlsx - Sheet1.csv")

names(info_about_monitoring_stations)
info_about_monitoring_stations <- info_about_monitoring_stations %>% 
  rename(Station=site_name)

daily_data <- left_join(daily_data, info_about_monitoring_stations)

daily_data <- daily_data %>%
  select(-`AURN Pollutants Measured`)

#match lat-long to la, for GB regions

#convert points data to a sf, with geometry based on those two column names
#with an adjustment to make sure points are WGS84(proper long-lat) - crs must be matched in both dataframes

match_up  <- sf::st_as_sf(daily_data, coords=c("longitude","latitude"), crs="+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs")

#read in geography of local authority boundaries as a shapefile - here ultra generalised
map <-  read_sf("LA_districts/Local_Authority_Districts_December_2017_Ultra_Generalised_Clipped_Boundaries_in_Great_Britain.shp")
#transform that shapefile to have the same crs pattern for curvature of the earth as the points sf
#this conversion can be tricky to do later for some reason
map <- st_transform(map, "+proj=longlat +ellps=WGS84 +datum=WGS84")
#change the shp file into a sf (they are different formats but must be sf for later steps)
map <- sf::st_as_sf(map)
#select out the la names and geometry
map <- map %>% 
  select(lad17nm,geometry)
#check str of map is sound
str(map) 

#plot both sf as simultaenous layers to visually check overlap
plot(map, reset=FALSE)
plot(match_up, add=TRUE, reset=FALSE, pch=16, col="red", cex=1.5)

#check st_intersects returns an integer (if empty, then no overlap working)
int <- sf::st_intersects(map, match_up)
int

#create a new object of the points data matched onto the area for which it intersects in the la layer
stations_match <- match_up %>% mutate(
  intersection = as.integer(st_intersects(match_up , map))
  , area = map$lad17nm[intersection])

#quick view of tibble
stations_match

#are the remaining points all in NI?

stations_unmatched <- stations_match %>% 
  filter(is.na(area))

#yes they are

#repair to df to allow tidyverse functions (still has hangover sf properties)
stations_match <- as.data.frame(stations_match)
stations_match <- stations_match %>% 
  select(-intersection) %>% 
  rename(la=area)

#fix known issues in la lookup before match

BBC_England_Wales_regions_radio_local_authority_lookup <- read_csv("BBC regions_radio_local authority lookup - Sheet1.csv")

names(BBC_England_Wales_regions_radio_local_authority_lookup)
BBC_England_Wales_regions_radio_local_authority_lookup <- BBC_England_Wales_regions_radio_local_authority_lookup %>% 
  rename(la=LA_name)

stations_match$la <- gsub("Bristol, City of", "Bristol", stations_match$la)

daily_data_matched <- left_join(stations_match, BBC_England_Wales_regions_radio_local_authority_lookup)

#repair Northern Ireland non-matches (temp fix)
#find non-matched(all NI), insert NI as govt region, then replace
daily_data_NI <- daily_data_matched %>% 
  filter(is.na(la)) %>%
  mutate(Northern_Ireland="Northern Ireland") %>% 
  select(-govt_region) %>% 
  rename(govt_region=Northern_Ireland)

daily_data_matched <- daily_data_matched %>% 
  filter(!is.na(la))

daily_data_matched <- rbind(daily_data_NI, daily_data_matched)

#insert year column, as for march comparison:

daily_data_matched <- daily_data_matched %>% 
  mutate(year=substrRight(Date,4))

#temporarily remove radio to leave regions for England/Wales/NI note:

daily_data_small <- daily_data_matched %>% 
  select(-bbc_radio_station,-bbc_online, -bbc_region, -bbc_region_uses_gov_name, -Notes)

  
```

